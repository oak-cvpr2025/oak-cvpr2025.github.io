<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="OAK: Open Ad-hoc Categorization with Contextualized Feature Learning (CVPR 2025)">
  <meta property="og:title" content="OAK: Open Ad-hoc Categorization with Contextualized Feature Learning (CVPR 2025)"/>
  <meta property="og:description" content="We introduce open ad-hoc categorization (OAK), a novel task requiring discovery of novel classes across diverse contexts, tackled by learning contextualized features with CLIP."/>
  <meta property="og:url" content="https://oak-cvpr2025.github.io/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="OAK: Open Ad-hoc Categorization with Contextualized Feature Learning (CVPR 2025)">
  <meta name="twitter:description" content="We introduce open ad-hoc categorization (OAK), a novel task requiring discovery of novel classes across diverse contexts, tackled by learning contextualized features with CLIP.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Ad-hoc Categorization, Open-world Classification, Category Discovery, CLIP, Computer Vision, CVPR 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>OAK: Open Ad-hoc Categorization with Contextualized Feature Learning (CVPR 2025)</title>

  <!-- Favicon and apple touch icon -->
  <link rel="icon" type="image/x-icon" href="static/images/favicon_io/favicon.ico">
  <link rel="apple-touch-icon" sizes="180x180" href="static/images/favicon_io/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="static/images/favicon_io/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="static/images/favicon_io/favicon-16x16.png">
  <link rel="manifest" href="static/images/favicon_io/site.webmanifest">

  <!-- Google fonts -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><img src="static/images/favicon_io/favicon-32x32.png" />OAK: Open Ad-hoc Categorization with Contextualized Feature Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block"><a href="https://wayne2wang.github.io/" target="_blank">Zilin Wang</a><sup>* 1</sup>,</span>
              <span class="author-block"><a href="https://sites.google.com/view/sangwoomo" target="_blank">Sangwoo Mo</a><sup>* 1</sup>,</span>
              <span class="author-block"><a href="https://web.eecs.umich.edu/~stellayu/" target="_blank">Stella X. Yu</a><sup>1,2</sup>,</span>
              <span class="author-block"><a href="https://www.linkedin.com/in/sima-behpour-95037713b/" target="_blank">Sima Behpour</a><sup>3</sup>,</span>
              <span class="author-block"><a href="https://www.liu-ren.com/" target="_blank">liu Ren</a><sup>3</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Michigan, <sup>2</sup>UC Berkeley, <sup>3</sup>Bosch Center for AI<br>CVPR 2025</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->

                <!-- Supplementary PDF link -->
                <!-- <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"  disabled class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <!-- Poster PDF link -->
                <span class="link-block">
                  <a href="https://cvpr.thecvf.com/media/PosterPDFs/CVPR%202025/34699.png?t=1748965972.857557" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Poster</span>
                  </a>
                </span>

                <!-- Video link -->
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=YOUR_VIDEO_ID" target="_blank" disabled class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-video"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="https://github.com/Wayne2Wang/OAK" target="_blank" disabled class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              </div>

            <!-- <p>
              <strong>TL;DR:</strong> We introduce open ad-hoc categorization (OAK), a novel task requiring discovery of novel classes across diverse contexts, and tackle it by learning contextualized features with CLIP.
            </p> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser figure -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <p>
          <strong>TL;DR:</strong> We introduce open ad-hoc categorization (OAK), a novel task requiring discovery of novel classes across diverse contexts, and tackle it by learning contextualized features with CLIP.
      </p>
      <br>
      <p>
        <img src="static/images/teaser.png" alt="Open ad-hoc categorization" class="blend-img-background center-image" style="max-width: 100%; height: auto;" />
      </p>
      <br>
      <p>
        We study open ad-hoc categorization (OAK) such as <em>things to sell at a garage sale</em> to achieve a specific goal (<em>selling unwanted items</em>).
        Given the context <em>garage sale</em>, <span style="color: rgb(0,200,0);">labeled exemplars</span> such as <span style="color: rgb(0,200,0);"><em>shoes</em></span>, we need to recognize all items in the scene that <em>can be sold at the garage sale</em>, including novel ones.
        Supervised models like CLIP focus on <span style="color: rgb(30,144,255);">1) closed-world generalization</span>, recognizing other <span style="color: rgb(30,144,255);"><em>shoes</em></span>.
        <span style="color: rgb(248,186,0);">2) novel semantic categories</span> can be discovered by contextual expansion from <span style="color: rgb(0,200,0);"><em>shoes</em></span> to <span style="color: rgb(248,186,0);"><em>hats</em></span>.
        Unsupervised methods like GCD discover <span style="color: rgb(255, 34, 0);">3) novel visual clusters</span>, identifying <span style="color: rgb(255, 34, 0);"><em>suitcases</em></span>.
      </p>
    </div>
  </div>
</section>

<!-- End teaser figure -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Adaptive categorization of visual scenes is essential for AI agents to handle changing tasks.  
            Unlike fixed common categories for plants or animals, ad-hoc categories, such as things to sell at a garage sale, are created dynamically to achieve specific tasks.  
            We study open ad-hoc categorization, where the goal is to infer novel concepts and categorize images based on a given context, a small set of labeled exemplars, and some unlabeled data.
          </p>
          <p>
            We have two key insights: 
            1) recognizing ad-hoc categories relies on the same perceptual processes as common categories; 
            2) novel concepts can be discovered semantically by expanding contextual cues or visually by clustering similar patterns.  
            We propose OAK, a simple model that introduces a single learnable context token into CLIP, trained with CLIP's objective of aligning visual and textual features and GCD's objective of clustering similar images.
          </p>
          <p>
            On Stanford and Clevr-4 datasets, OAK consistently achieves the state-of-art in accuracy and concept discovery across multiple categorizations, including 87.4% novel accuracy on Stanford Mood, surpassing CLIP and GCD by over 50%. 
            Moreover, OAK generates interpretable saliency maps, focusing on hands for Action, faces for Mood, and backgrounds for Location, promoting transparency and trust while enabling accurate and flexible categorization.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Task formulation -->

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Task: Open Set Discovery + Context Switching</h2>
          <img src="static/images/task.png" alt="OAK task formulation" class="blend-img-background center-image" />
          <p>
            Open ad-hoc categorization (OAK) learns diverse categorization rules, dynamically adapting to varying user needs at hand. The same image should be recognized differently depending on context, such as <em>drinking</em> for action and <em>residential</em> for location. We emphasize the ability to switch between multiple contexts in OAK.
            Specifically, given 1) a context defined by classes, 2) a few labeled images, and 3) a set of unlabeled images, OAK holistically reasons over <span style="color: rgb(0,200,0);">labeled</span> and unlabeled images, spanning both <span style="color: rgb(30,144,255);">known</span> and <span style="color: rgb(255,128,0);">novel</span> classes, to infer novel concepts and propagate labels across the entire dataset.
            We show the class names of labeled images in the color box and unlabeled images inside the parentheses, reflecting that the unlabeled class names are not available, only the images.
            OAK introduces unique challenges beyond generalized category discovery (GCD), requiring adaptation to diverse ad-hoc categorization rules based on context.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- End task formulation -->



<!-- Method -->

<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Method: Context Tokens to CLIP with Visual Clustering</h2>
          <img src="static/images/method.png" alt="OAK method" class="blend-img-background center-image" />
          <p>
            OAK learns contextualized features while preserving the foundations of perception of CLIP by introducing context tokens that modulate the frozen ViT encoder, achieving context-aware attention. 
            This contextualized feature learning follows two key principles: 
            1) top-down text guidance, which leverages semantic knowledge from known class names, and 
            2) bottom-up image clustering, which captures visual similarity to infer categorization rules. 
            OAK aligns visual clusters with semantic cues by inferring pseudo-labels using the text encoder and refining clusters accordingly. 
            This unified approach outperforms individual methods such as CLIP and GCD, effectively combining their strengths.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- End method -->




<!-- Naming -->

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Naming Novel Concepts: CLIP-ZS with LLM-Inferred Vocabulary</h2>
          <p>
          <strong>Prompt: </strong><i>I have a dataset of images from the following classes: <strong>[KNOWN_CLASSES]</strong>. What are the most possible classes that will also be included in this dataset? Give me <strong>[NUMBER_OF_NOVEL_CLASSES]</strong> class names, only return class names separated by commas. Include quotation marks for each one.</i>
          </p>
          <img src="static/images/naming_results.png" alt="OAK Naming Results" class="blend-img-background center-image" />
          <p>
            Class names associated with novel visual clusters from our models show that it identifies reasonable words for contexts familiar to CLIP (Action, Location, Color, Shape), but less accurate ones for less familiar contexts (Mood, Texture, Count). 
            We include full lists and visual examples in <em>Supplementary Material C</em>. 
          </p>
          <div style="display: flex; justify-content: center;">
            <img src="static/images/naming_example.png" alt="OAK Naming Examples" class="blend-img-background center-image" style="width:60%;" />
          </div>
          <p>
            Visual examples with true and predicted class names, showing OAK assigns reasonable labels based on visual cues, such as <em>jumping</em> for people appearing <em>dancing</em>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- End naming -->


<!-- Contextualized Features -->

<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Novel Concepts Arise from Contextualized Features</h2>
          <img src="static/images/contextualized_features.png" alt="OAK Contextualized Features" class="blend-img-background center-image" />
          <p>
            t-SNE visualization of CLIP visual features and nearest neighbor examples from CLIP (row 1) and OAK (row 2) on Clevr-4 Shape, Color, Texture, Count. 
            CLIP features only work on the context that aligns with its training data such as Shape, while OAK contextualizes the feature space so that both known and novel classes form meaningful groups.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- End contextualized features -->


<!-- Contextualized Attention -->

<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Contextualized Attention Highlights Correct Regions</h2>
          <img src="static/images/contextualized_attention.png" alt="OAK Contextualized Attention" class="blend-img-background center-image" />
          <p>
            Saliency maps on the Stanford dataset show that OAK focuses on relevant regions of images for different contexts, while GCD often distracts to arbitrary regions.
            We select two samples predicted correctly by OAK across all contexts and visualize the saliency maps of CLIP, GCD, and OAK using the approach of Chefer et al., guided by the predicted class, except for CLIP, where we use an empty string.
            <span style="color:#68CE4B;">Correct</span> and <span style="color: #BC2D1F;">incorrect</span> predictions are colored accordingly.
            OAK focuses on human behaviors, like <em>hand movements</em> for Action, covers the <em>entire scene</em> for Location, and highlights a <em>human face</em> for Mood, closely aligning with human intuition.
            GCD produces reasonable saliency maps for Action, as seen in the <em>phoning</em> example, but confuses <em>fixing a bike</em> with <em>riding a bike</em> by focusing on the bike rather than human behavior.
            CLIP focuses on salient objects such as <em>humans</em>, without adapting to each context.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- End contextualized attention -->


<!-- Effectiveness and Versatility -->

<section class="section hero is-small is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-3">Contextualized Attention Highlights Correct Regions</h2>
          <img src="static/images/results.png" alt="OAK Main Results" class="blend-img-background center-image" />
          <p>
            OAK consistently outperforms open-vocabulary classification (row group 1) and visual clustering (row group 2) baselines, particularly on novel classes and prediction consistency. 
            This advantage is most pronounced in less familiar contexts like Mood. 
            We report known, novel, and overall accuracies for each context, including Omni accuracy, with best results in bold.
            CLIP-ZS + LLM vocab performs poorly on novel classes, revealing the limitation of using class names alone.
            GCD addresses this by clustering visual features, but OAK goes further by contextualizing them with CLIP's semantic knowledge, achieving a 50% gain over both CLIP and GCD in Mood. 
            In terms of the Omni accuracy, OAK achieves 70.3% overall accuracy, outperforming all baselines by 2-30% and demonstrating consistency across contexts.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- End effectiveness and versatility -->




<!--BibTex citation -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>. 
          </p>

        </div>
      </div>
      <!-- Statcounter tracking code -->
      <a href='https://mapmyvisitors.com/web/1bycn'  title='Visit tracker'><img src='https://mapmyvisitors.com/map.png?cl=080808&w=a&t=m&d=9N5EWw_9GyCyb021Le-wsVAUyV_0w5C_AGHCDB_K3ok&co=ffffff&ct=808080'/></a>
    </div>
  </div>
</footer>

  </body>
  </html>
